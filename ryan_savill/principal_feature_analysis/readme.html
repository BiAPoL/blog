

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Principal Feature Analysis &#8212; BiA-PoL blog</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ryan_savill/principal_feature_analysis/readme';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Background Subtraction" href="../03_background_subtraction/readme.html" />
    <link rel="prev" title="Installing Microsoft buildtools on Windows" href="../../robert_haase/ms_build_tools/readme.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/biapol_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/biapol_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Blog Posts</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../mara_lampert/getting_started_with_mambaforge_and_python/readme.html">Getting started with Mambaforge and Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../till_korten/devbio-napari_cluster/readme.html">Using GPU-accelerated image processing on the TUD HPC cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../till_korten/devbio-napari_cluster_setup/readme.html">Setting up GPU-accelerated image processing on the TUD HPC cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../marcelo_zoccoler/omero_scripts/readme.html">Running Deep-Learning Scripts in the BiA-PoL Omero Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../marcelo_zoccoler/mini_sabbatical_rike/Readme.html">Mini-Sabbatical Experience at Bia-PoL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../johannes_mueller/anaconda_getting_started/Readme.html">Getting started with Anaconda and Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../johannes_mueller/entry_sphinx/Readme.html">Automated package documentation with Sphinx</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../johannes_mueller/entry_user_inter/Readme.html">Custom user interfaces for Python (Part 1)</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../johannes_mueller/entry_user_interf2/Readme.html">Custom user interfaces for Python (Part 2)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../marcelo_zoccoler/entry_user_interf3/Readme.html">Custom user interfaces for Python (Part 3)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../marcelo_zoccoler/entry_user_interf4/Readme.html">Custom user interfaces for Python (Part 4)</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../marcelo_zoccoler/jamovi/jamovi.html">Jamovi: statistical analysis made visual and easy (powered with R)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../robert_haase/ms_build_tools/readme.html">Installing Microsoft buildtools on Windows</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Principal Feature Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_background_subtraction/readme.html">Background Subtraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_intro_to_skimage/readme.html">Introduction to Image Analysis Basics in Python with Scikit Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../robert_haase/stardist_gpu/readme.html">Using StarDist in napari with GPU-support in Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../01_intro_to_python/readme.html">Introduction to Using Python for Image Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../robert_haase/cupy_cucim/readme.html">GPU-accelerated image processing using cupy and cucim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../robert_haase/browsing_idr/readme.html">Browsing the Open Microscopy Image Data Resource with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../robert_haase/clesperanto_google_colab/readme.html">GPU-accelerated image processing in the cloud using Google Colab and clEsperanto</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">About the Blog</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../robert_haase/why_we_blog/readme.html">Why we blog</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../people.html">BiA-PoL blog authors</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mara_lampert/readme.html">Mara Lampert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../johannes_mueller/Readme.html">Johannes Müller</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../marcelo_zoccoler/readme.html">Marcelo Zoccoler</a></li>


<li class="toctree-l2"><a class="reference internal" href="../readme.html">Ryan Savill</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../robert_haase/readme.html">Robert Haase</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../imprint.html">Imprint</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/BiAPoL/blog" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/BiAPoL/blog/issues/new?title=Issue%20on%20page%20%2Fryan_savill/principal_feature_analysis/readme.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/ryan_savill/principal_feature_analysis/readme.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Principal Feature Analysis</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="principal-feature-analysis">
<h1>Principal Feature Analysis<a class="headerlink" href="#principal-feature-analysis" title="Permalink to this heading">#</a></h1>
<p><a class="reference internal" href="../readme.html"><span class="doc std std-doc">Ryan Savill</span></a>, July 8th 2021</p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this heading">#</a></h2>
<p>Principal feature analysis (PFA) is a method for selecting a subset of features that describe most of the variability in the dataset based on <a class="reference external" href="http://venom.cs.utsa.edu/dmz/techrep/2007/CS-TR-2007-011.pdf">this paper</a> by Y. Lu et al. published in ACM, 2007. Most of the code for the implementation below was provided in a <a class="reference external" href="https://stats.stackexchange.com/questions/108743/methods-in-r-or-python-to-perform-feature-selection-in-unsupervised-learning/203978#203978">stackexchange comment section</a> and is greatly appreciated!</p>
<p>This sounds oddly similar to principal component analysis (PCA), which is no coincidence as the methodologies are intertwined. PCA also does a similar thing but instead of choosing features that describe the variability to a certain threshold we choose a subset of principal components. This is not always optimal and there are valid choices for working with the features instead of the principal components as the principal components are always a transformed form of the data.</p>
<p>If you have not heard of PCA before I would advise you to check out <a class="reference external" href="https://www.youtube.com/watch?v=FgakZw6K1QQ">this video</a>. If you (like me just a few days ago) have an understanding of the principle of PCA but don’t understand the math behind it I’d advise you to take some time to grasp the concepts. I found a great series of videos that explain all necessary parts of the puzzle that is PCA. Each video is just about 10 minutes long and explained quite well, even if your math lectures have been a few years ago!</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=X78tLBY3BMk">Vector Projections</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=glaiP222JWA">Eigenvectors and Eigenvalues</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=e73033jZTCI">Derivative of a Matrix</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=6oZT72-nnyI">Lagrange Multipliers</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=152tSYtiQbw">Covariance Matrix</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=dhK8nbtii6I">PCA</a> (finally!)</p></li>
</ul>
<p>Once you have grasped these concepts this explanation should be a breeze!</p>
<p>Principal feature analysis takes advantage of some of the transformations in PCA to find out which features explain the variance. The first steps in PFA are to create the covariance matrix and find the orthonormal eigenvectors and sort them by their eigenvalues, creating a matrix A. This in effect is what PCA does as well since the principal components are the eigenvectors of the covariance matrix. We can output them in <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> with the command ‘.components_’.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># importing everything we might need and more:</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">decomposition</span>
<span class="kn">import</span> <span class="nn">tribolium_clustering</span> <span class="k">as</span> <span class="nn">tc</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># function for labelling eigenvectors so it is readable:</span>
<span class="k">def</span> <span class="nf">pca_comp_inf_dataframe</span><span class="p">(</span><span class="n">pca_object</span><span class="p">,</span><span class="n">regprops_df</span><span class="p">,</span><span class="n">n_components</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
    <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
    <span class="n">features</span> <span class="o">=</span> <span class="n">regprops_df</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="n">component_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)]</span>
    <span class="n">comps</span> <span class="o">=</span> <span class="n">pca_object</span><span class="o">.</span><span class="n">components_</span>
    <span class="n">df_comps</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">comps</span><span class="p">,</span><span class="n">index</span> <span class="o">=</span> <span class="n">component_names</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">features</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">df_comps</span>

<span class="c1"># loading our test dataset</span>
<span class="n">location_prefix</span> <span class="o">=</span> <span class="s1">&#39;data/folder/&#39;</span>

<span class="n">timepoint</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">regionpropspath</span> <span class="o">=</span> <span class="n">location_prefix</span> <span class="o">+</span> <span class="s1">&#39;Master Thesis//First Coding Tries//regionprops_all_timepoints_lund//regprops t</span><span class="si">{}</span><span class="s1">.csv&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">timepoint</span><span class="p">)</span>
<span class="n">regprops</span> <span class="o">=</span> <span class="n">tc</span><span class="o">.</span><span class="n">readcsv_as_cl_input</span><span class="p">(</span><span class="n">regionpropspath</span><span class="p">)</span>

<span class="c1"># redefinition of our dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">regprops</span>

<span class="c1"># standardscaling the dataset is a prerequisite for PCA</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># fitting our PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># getting our orthonormal eigenvectors:</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># labelling that matrix for ease of use</span>
<span class="n">A_readable</span> <span class="o">=</span> <span class="n">pca_comp_inf_dataframe</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span><span class="n">regprops</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">regprops</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

<span class="n">A_readable</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>No Predictions in Regionprops of data/folder/Master Thesis/First Coding Tries/regionprops_all_timepoints_lund/regprops t9.csv
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
      <th>PC5</th>
      <th>PC6</th>
      <th>PC7</th>
      <th>PC8</th>
      <th>PC9</th>
      <th>PC10</th>
      <th>PC11</th>
      <th>PC12</th>
      <th>PC13</th>
      <th>PC14</th>
      <th>PC15</th>
      <th>PC16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>area</th>
      <td>0.317564</td>
      <td>0.035871</td>
      <td>0.001643</td>
      <td>-0.060553</td>
      <td>-0.099831</td>
      <td>0.060197</td>
      <td>0.121308</td>
      <td>-0.070996</td>
      <td>-0.059213</td>
      <td>-0.200579</td>
      <td>0.358877</td>
      <td>0.496485</td>
      <td>0.533148</td>
      <td>0.376162</td>
      <td>0.126583</td>
      <td>-0.007435</td>
    </tr>
    <tr>
      <th>centroid-0</th>
      <td>0.031897</td>
      <td>0.394748</td>
      <td>-0.671168</td>
      <td>0.066552</td>
      <td>-0.171701</td>
      <td>-0.245463</td>
      <td>-0.296170</td>
      <td>-0.423315</td>
      <td>-0.143398</td>
      <td>0.086719</td>
      <td>0.030823</td>
      <td>-0.041805</td>
      <td>-0.005947</td>
      <td>0.007349</td>
      <td>0.026808</td>
      <td>0.005131</td>
    </tr>
    <tr>
      <th>centroid-1</th>
      <td>0.008123</td>
      <td>0.382147</td>
      <td>0.488175</td>
      <td>0.230320</td>
      <td>-0.613868</td>
      <td>-0.143166</td>
      <td>-0.356966</td>
      <td>0.192595</td>
      <td>0.011508</td>
      <td>-0.013926</td>
      <td>0.001356</td>
      <td>0.008315</td>
      <td>-0.007246</td>
      <td>0.000669</td>
      <td>-0.015394</td>
      <td>-0.000195</td>
    </tr>
    <tr>
      <th>centroid-2</th>
      <td>0.240592</td>
      <td>-0.076633</td>
      <td>-0.435728</td>
      <td>0.075588</td>
      <td>-0.140601</td>
      <td>-0.327701</td>
      <td>0.114531</td>
      <td>0.634164</td>
      <td>0.393689</td>
      <td>-0.174994</td>
      <td>-0.084351</td>
      <td>0.026689</td>
      <td>-0.018534</td>
      <td>-0.022879</td>
      <td>-0.062223</td>
      <td>-0.007310</td>
    </tr>
    <tr>
      <th>feret_diameter_max</th>
      <td>0.289769</td>
      <td>-0.012282</td>
      <td>-0.032635</td>
      <td>0.181363</td>
      <td>-0.160207</td>
      <td>0.380691</td>
      <td>0.084649</td>
      <td>-0.211788</td>
      <td>0.439432</td>
      <td>0.320636</td>
      <td>-0.163147</td>
      <td>0.056981</td>
      <td>-0.363715</td>
      <td>0.448382</td>
      <td>0.007851</td>
      <td>-0.012093</td>
    </tr>
    <tr>
      <th>major_axis_length</th>
      <td>0.301739</td>
      <td>0.002370</td>
      <td>-0.021720</td>
      <td>0.113154</td>
      <td>-0.135457</td>
      <td>0.346191</td>
      <td>0.039834</td>
      <td>-0.195065</td>
      <td>0.315525</td>
      <td>0.010066</td>
      <td>0.049534</td>
      <td>-0.247254</td>
      <td>0.419644</td>
      <td>-0.608955</td>
      <td>-0.086439</td>
      <td>0.013967</td>
    </tr>
    <tr>
      <th>minor_axis_length</th>
      <td>0.308058</td>
      <td>0.048526</td>
      <td>0.032858</td>
      <td>-0.149205</td>
      <td>-0.075049</td>
      <td>0.130833</td>
      <td>0.027726</td>
      <td>-0.170614</td>
      <td>-0.094954</td>
      <td>-0.740310</td>
      <td>0.122889</td>
      <td>-0.086970</td>
      <td>-0.489703</td>
      <td>-0.077289</td>
      <td>-0.034946</td>
      <td>-0.005721</td>
    </tr>
    <tr>
      <th>solidity</th>
      <td>0.139750</td>
      <td>0.213562</td>
      <td>0.179860</td>
      <td>-0.784037</td>
      <td>0.109756</td>
      <td>-0.245965</td>
      <td>-0.162139</td>
      <td>-0.093897</td>
      <td>0.399242</td>
      <td>0.142184</td>
      <td>-0.016268</td>
      <td>-0.005970</td>
      <td>0.007210</td>
      <td>-0.012330</td>
      <td>0.005534</td>
      <td>-0.001295</td>
    </tr>
    <tr>
      <th>mean_intensity</th>
      <td>-0.298859</td>
      <td>-0.031721</td>
      <td>-0.044582</td>
      <td>-0.053140</td>
      <td>-0.290490</td>
      <td>-0.081776</td>
      <td>0.366782</td>
      <td>-0.070965</td>
      <td>0.147539</td>
      <td>0.210029</td>
      <td>0.577876</td>
      <td>0.280034</td>
      <td>-0.316568</td>
      <td>-0.304428</td>
      <td>0.055622</td>
      <td>0.001809</td>
    </tr>
    <tr>
      <th>max_intensity</th>
      <td>-0.223891</td>
      <td>0.445127</td>
      <td>-0.015072</td>
      <td>-0.078562</td>
      <td>-0.095429</td>
      <td>0.088448</td>
      <td>0.504917</td>
      <td>-0.064260</td>
      <td>-0.004226</td>
      <td>-0.155993</td>
      <td>-0.573306</td>
      <td>0.308846</td>
      <td>0.074610</td>
      <td>-0.118217</td>
      <td>-0.036991</td>
      <td>0.002788</td>
    </tr>
    <tr>
      <th>min_intensity</th>
      <td>-0.248677</td>
      <td>-0.277709</td>
      <td>-0.003527</td>
      <td>-0.151868</td>
      <td>-0.443942</td>
      <td>-0.187537</td>
      <td>0.304523</td>
      <td>-0.223142</td>
      <td>0.067566</td>
      <td>-0.152249</td>
      <td>-0.039673</td>
      <td>-0.525101</td>
      <td>0.223898</td>
      <td>0.328584</td>
      <td>-0.005177</td>
      <td>-0.002429</td>
    </tr>
    <tr>
      <th>image_stdev</th>
      <td>-0.124373</td>
      <td>0.597732</td>
      <td>-0.026935</td>
      <td>0.059831</td>
      <td>0.275986</td>
      <td>0.222842</td>
      <td>0.207255</td>
      <td>0.258350</td>
      <td>0.080221</td>
      <td>-0.016470</td>
      <td>0.376158</td>
      <td>-0.432514</td>
      <td>0.043550</td>
      <td>0.222652</td>
      <td>0.025638</td>
      <td>-0.004268</td>
    </tr>
    <tr>
      <th>avg distance of 2 closest points</th>
      <td>0.312403</td>
      <td>0.040115</td>
      <td>0.025366</td>
      <td>-0.113510</td>
      <td>-0.083929</td>
      <td>-0.065002</td>
      <td>0.223752</td>
      <td>0.060992</td>
      <td>-0.355553</td>
      <td>0.261325</td>
      <td>0.057316</td>
      <td>-0.050857</td>
      <td>-0.037707</td>
      <td>0.038347</td>
      <td>-0.716577</td>
      <td>0.324435</td>
    </tr>
    <tr>
      <th>avg distance of 3 closest points</th>
      <td>0.315714</td>
      <td>0.031638</td>
      <td>0.028109</td>
      <td>-0.084834</td>
      <td>-0.084732</td>
      <td>-0.064296</td>
      <td>0.212046</td>
      <td>0.076844</td>
      <td>-0.308744</td>
      <td>0.228016</td>
      <td>-0.034887</td>
      <td>-0.121603</td>
      <td>-0.049914</td>
      <td>-0.068178</td>
      <td>0.120108</td>
      <td>-0.802665</td>
    </tr>
    <tr>
      <th>avg distance of 4 closest points</th>
      <td>0.316944</td>
      <td>0.023361</td>
      <td>0.033518</td>
      <td>-0.059729</td>
      <td>-0.078356</td>
      <td>-0.068524</td>
      <td>0.204538</td>
      <td>0.092240</td>
      <td>-0.253916</td>
      <td>0.188049</td>
      <td>-0.088972</td>
      <td>-0.156060</td>
      <td>-0.072032</td>
      <td>-0.097133</td>
      <td>0.660690</td>
      <td>0.499889</td>
    </tr>
    <tr>
      <th>touching neighbor count</th>
      <td>-0.190434</td>
      <td>-0.078438</td>
      <td>-0.284396</td>
      <td>-0.437245</td>
      <td>-0.339335</td>
      <td>0.594522</td>
      <td>-0.243295</td>
      <td>0.334427</td>
      <td>-0.201843</td>
      <td>0.052019</td>
      <td>-0.024418</td>
      <td>0.025692</td>
      <td>0.012726</td>
      <td>0.013417</td>
      <td>0.028237</td>
      <td>0.006079</td>
    </tr>
  </tbody>
</table>
</div>
<p>Now we have a massive matrix of our eigenvectors sorted by eigenvalues, but since every feature has equal influence on the eigenvectors we need to take a subset of matrix <code class="docutils literal notranslate"><span class="pre">A</span></code> called <code class="docutils literal notranslate"><span class="pre">A_q</span></code>. <code class="docutils literal notranslate"><span class="pre">q</span></code> represents the number of eigenvectors we take into account and the value of <code class="docutils literal notranslate"><span class="pre">q</span></code> will influence how much variance we can explain with the subset of the principal components. We can turn this question around and set a threshold of how much variance should at least be explained and then find the corresponding subset <code class="docutils literal notranslate"><span class="pre">A-q</span></code> of matrix <code class="docutils literal notranslate"><span class="pre">A</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># this shows us how much variance each principal component explains</span>
<span class="n">explained_variance</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Explained Variances&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="c1"># this shows us how much variance the subset up to the i&#39;th index explains</span>
<span class="n">cumulative_expl_var</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">))]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cumulative Explained Variance&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cumulative_expl_var</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="c1"># now we iterate through the cumulative explained variance until we reach a threshold that we define</span>
<span class="n">q_variance_limit</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cumulative_expl_var</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="n">q_variance_limit</span><span class="p">:</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">i</span>
        <span class="k">break</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The Subset will include the first </span><span class="si">{}</span><span class="s1"> Eigenvectors&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">q</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Explained Variances
[5.89549241e-01 1.24817549e-01 7.44073363e-02 6.06447683e-02
 5.05242728e-02 3.80817718e-02 2.29718719e-02 1.43923774e-02
 1.16341513e-02 5.35585488e-03 2.50361397e-03 1.82220555e-03
 1.48239356e-03 1.06126637e-03 6.85699035e-04 6.56265602e-05]

Cumulative Explained Variance
[0.5895492409109632, 0.7143667903013323, 0.7887741265865403, 0.8494188949293884, 0.8999431677426732, 0.9380249394988094, 0.9609968114170524, 0.9753891887742816, 0.9870233400706391, 0.992379194950718, 0.9948828089223744, 0.9967050144743802, 0.9981874080386469, 0.9992486744047498, 0.9999343734397964, 0.9999999999999999]

The Subset will include the first 5 Eigenvectors
</pre></div>
</div>
<p>With our q defined we can get the the subset matrix of eigenvectors A_q:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A_q</span> <span class="o">=</span> <span class="n">A</span><span class="p">[:,:</span><span class="n">q</span><span class="p">]</span>

<span class="n">A_q_readable</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">A_q</span><span class="p">,</span><span class="n">index</span> <span class="o">=</span> <span class="n">regprops</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">q</span><span class="p">)])</span>

<span class="n">A_q_readable</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
      <th>PC5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>area</th>
      <td>0.317564</td>
      <td>0.035871</td>
      <td>0.001643</td>
      <td>-0.060553</td>
      <td>-0.099831</td>
    </tr>
    <tr>
      <th>centroid-0</th>
      <td>0.031897</td>
      <td>0.394748</td>
      <td>-0.671168</td>
      <td>0.066552</td>
      <td>-0.171701</td>
    </tr>
    <tr>
      <th>centroid-1</th>
      <td>0.008123</td>
      <td>0.382147</td>
      <td>0.488175</td>
      <td>0.230320</td>
      <td>-0.613868</td>
    </tr>
    <tr>
      <th>centroid-2</th>
      <td>0.240592</td>
      <td>-0.076633</td>
      <td>-0.435728</td>
      <td>0.075588</td>
      <td>-0.140601</td>
    </tr>
    <tr>
      <th>feret_diameter_max</th>
      <td>0.289769</td>
      <td>-0.012282</td>
      <td>-0.032635</td>
      <td>0.181363</td>
      <td>-0.160207</td>
    </tr>
    <tr>
      <th>major_axis_length</th>
      <td>0.301739</td>
      <td>0.002370</td>
      <td>-0.021720</td>
      <td>0.113154</td>
      <td>-0.135457</td>
    </tr>
    <tr>
      <th>minor_axis_length</th>
      <td>0.308058</td>
      <td>0.048526</td>
      <td>0.032858</td>
      <td>-0.149205</td>
      <td>-0.075049</td>
    </tr>
    <tr>
      <th>solidity</th>
      <td>0.139750</td>
      <td>0.213562</td>
      <td>0.179860</td>
      <td>-0.784037</td>
      <td>0.109756</td>
    </tr>
    <tr>
      <th>mean_intensity</th>
      <td>-0.298859</td>
      <td>-0.031721</td>
      <td>-0.044582</td>
      <td>-0.053140</td>
      <td>-0.290490</td>
    </tr>
    <tr>
      <th>max_intensity</th>
      <td>-0.223891</td>
      <td>0.445127</td>
      <td>-0.015072</td>
      <td>-0.078562</td>
      <td>-0.095429</td>
    </tr>
    <tr>
      <th>min_intensity</th>
      <td>-0.248677</td>
      <td>-0.277709</td>
      <td>-0.003527</td>
      <td>-0.151868</td>
      <td>-0.443942</td>
    </tr>
    <tr>
      <th>image_stdev</th>
      <td>-0.124373</td>
      <td>0.597732</td>
      <td>-0.026935</td>
      <td>0.059831</td>
      <td>0.275986</td>
    </tr>
    <tr>
      <th>avg distance of 2 closest points</th>
      <td>0.312403</td>
      <td>0.040115</td>
      <td>0.025366</td>
      <td>-0.113510</td>
      <td>-0.083929</td>
    </tr>
    <tr>
      <th>avg distance of 3 closest points</th>
      <td>0.315714</td>
      <td>0.031638</td>
      <td>0.028109</td>
      <td>-0.084834</td>
      <td>-0.084732</td>
    </tr>
    <tr>
      <th>avg distance of 4 closest points</th>
      <td>0.316944</td>
      <td>0.023361</td>
      <td>0.033518</td>
      <td>-0.059729</td>
      <td>-0.078356</td>
    </tr>
    <tr>
      <th>touching neighbor count</th>
      <td>-0.190434</td>
      <td>-0.078438</td>
      <td>-0.284396</td>
      <td>-0.437245</td>
      <td>-0.339335</td>
    </tr>
  </tbody>
</table>
</div>
<p>If you have read the paper or are following along with the paper we have just completed step 3 of the explained algorithm. What is done now is that we perform clustering on the matrix A_q with K-means clustering. Essentially, we are trying to find clusters of features as features that are clustered in this matrix have similar influences on the principal components and are thus correlating and not telling us much. This way we can just choose the feature that is closest to the cluster centre and it will be chosen as a principal feature.</p>
<p>We can see that the choice of the cluster-number will determine how many features we choose. Y. Lu et al. recommend choosing the cluster-number: p, as: <code class="docutils literal notranslate"><span class="pre">p</span> <span class="pre">&gt;=</span> <span class="pre">q</span></code> because we can’t be completely sure that the variance that we chose is explained when <code class="docutils literal notranslate"><span class="pre">p</span> <span class="pre">=</span> <span class="pre">q</span></code>. We can just opt for a slightly larger <code class="docutils literal notranslate"><span class="pre">p</span></code> (arbitrary choice of 2  larger than <code class="docutils literal notranslate"><span class="pre">q</span></code>) but there is room for improvement in terms of an automated choice here OR it could be a parameter that we can implement later.</p>
<p>Since K-means-clustering is implemented well in <a class="reference external" href="https://scikit-learn.org/stable/modules/clustering.html">scikit-learn</a> this procedure is just a couple of lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span> <span class="n">q</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A_q</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">A_q</span><span class="p">)</span>
<span class="n">cluster_centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

<span class="n">clusters_readable</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">clusters</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">regprops</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Cluster&#39;</span><span class="p">])</span>
<span class="n">clusters_readable</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</pre></div>
</div>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>area</th>
      <td>2</td>
    </tr>
    <tr>
      <th>centroid-0</th>
      <td>5</td>
    </tr>
    <tr>
      <th>centroid-1</th>
      <td>6</td>
    </tr>
    <tr>
      <th>centroid-2</th>
      <td>1</td>
    </tr>
    <tr>
      <th>feret_diameter_max</th>
      <td>2</td>
    </tr>
    <tr>
      <th>major_axis_length</th>
      <td>2</td>
    </tr>
    <tr>
      <th>minor_axis_length</th>
      <td>2</td>
    </tr>
    <tr>
      <th>solidity</th>
      <td>4</td>
    </tr>
    <tr>
      <th>mean_intensity</th>
      <td>0</td>
    </tr>
    <tr>
      <th>max_intensity</th>
      <td>3</td>
    </tr>
    <tr>
      <th>min_intensity</th>
      <td>0</td>
    </tr>
    <tr>
      <th>image_stdev</th>
      <td>3</td>
    </tr>
    <tr>
      <th>avg distance of 2 closest points</th>
      <td>2</td>
    </tr>
    <tr>
      <th>avg distance of 3 closest points</th>
      <td>2</td>
    </tr>
    <tr>
      <th>avg distance of 4 closest points</th>
      <td>2</td>
    </tr>
    <tr>
      <th>touching neighbor count</th>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
<p>What can we see here? Well each feature is now part of a cluster (numbers 0 - 6, since we have chosen 7 clusters). What we can also see is that quite a few features are part of the same cluster( = 2), while some features are only part of a unique cluster without any other features. This means that these features do not correlate (much at least) with any other features and will be chosen by our feature analysis. The clusters which have 2 or more features as part of it are the clusters where a feature has to be chosen. This is decided by the distances the features have from the cluster centres, so let’s determine the distances from the cluster centres for each feature:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># initialising a dictionary to keep the distances in:</span>
<span class="n">dists</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

<span class="c1"># iterating through each feature (i) and it&#39;s cluster (c)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
    
    <span class="c1"># measuring the distance of each feature in A_q ([A_q[i, :]])</span>
    <span class="c1"># and the cluster centers [cluster_centers[c, :]]</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">([</span><span class="n">A_q</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]],</span> <span class="p">[</span><span class="n">cluster_centers</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="p">:]])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># each cluster is a key in the dictionary and the distance of each feature</span>
    <span class="c1"># belonging to that cluster is saved under this key as a tupule (feature_idx, distance)</span>
    <span class="n">dists</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">dist</span><span class="p">))</span>

<span class="n">dists</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>defaultdict(list,
            {2: [(0, 0.039527541499107074),
              (4, 0.22202861661398224),
              (5, 0.1470198671974888),
              (6, 0.13180725994276707),
              (12, 0.09346378648078914),
              (13, 0.06610212866152891),
              (14, 0.04944098356286365)],
             5: [(1, 0.0)],
             6: [(2, 0.0)],
             1: [(3, 0.0)],
             4: [(7, 0.0)],
             0: [(8, 0.217150845350435),
              (10, 0.2117088702247021),
              (15, 0.29315857787851873)],
             3: [(9, 0.21819421392343466), (11, 0.21819421392343452)]})
</pre></div>
</div>
<p>In our <code class="docutils literal notranslate"><span class="pre">dists</span></code> dictionary each cluster has a list of tuples associated with it. Each tuple contains:
(feature index, distance to cluster center)
We can see that for the clusters with only one tuple associated <code class="docutils literal notranslate"><span class="pre">(5,6,1,4)</span></code>, the features all have distance 0 since the cluster center IS the feature. For all other clusters we still have to determine which feature is closest to the cluster center. Once the features have been determined we can transform our original dataset to only include these features:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># here we are getting the indices of the features with the smallest distances </span>
<span class="c1"># to the cluster centers, essentially choosing which features to keep</span>
<span class="n">indices_</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sorted</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">dists</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>

<span class="c1"># transforming the input data to only include the features we just chose</span>
<span class="n">features_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">indices_</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this heading">#</a></h2>
<p>Since data science in python is mainly performed in <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a> we’ll stick to the conventions and create a class instead of a function for PFA. As stated above this is a slightly modified code provided by <a class="reference external" href="https://stats.stackexchange.com/users/76815/ulf-aslak">Ulf Aslak</a> on <a class="reference external" href="https://stats.stackexchange.com/questions/108743/methods-in-r-or-python-to-perform-feature-selection-in-unsupervised-learning">this stackexchange thread</a>. Thanks for your kind code donation!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PFA</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">diff_n_features</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">explained_var</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diff_n_features</span> <span class="o">=</span> <span class="n">diff_n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">explained_var</span> <span class="o">=</span> <span class="n">explained_var</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">:</span>
            <span class="n">explained_variance</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
            <span class="n">cumulative_expl_var</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">))]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cumulative_expl_var</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explained_var</span><span class="p">:</span>
                    <span class="n">q</span> <span class="o">=</span> <span class="n">i</span>
                    <span class="k">break</span>
                    
        <span class="n">A_q</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">[:,:</span><span class="n">q</span><span class="p">]</span>
        
        <span class="n">clusternumber</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">diff_n_features</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        
        <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span> <span class="n">clusternumber</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A_q</span><span class="p">)</span>
        <span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">A_q</span><span class="p">)</span>
        <span class="n">cluster_centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

        <span class="n">dists</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">([</span><span class="n">A_q</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]],</span> <span class="p">[</span><span class="n">cluster_centers</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="p">:]])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">dists</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">dist</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">indices_</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sorted</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">dists</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>    
        <span class="n">sc</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">:</span>
            <span class="n">explained_variance</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
            <span class="n">cumulative_expl_var</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">[:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">explained_variance</span><span class="p">))]</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cumulative_expl_var</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">explained_var</span><span class="p">:</span>
                    <span class="n">q</span> <span class="o">=</span> <span class="n">i</span>
                    <span class="k">break</span>
                    
        <span class="n">A_q</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">[:,:</span><span class="n">q</span><span class="p">]</span>
        
        <span class="n">clusternumber</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([</span><span class="n">q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">diff_n_features</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        
        <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span> <span class="n">clusternumber</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">A_q</span><span class="p">)</span>
        <span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">A_q</span><span class="p">)</span>
        <span class="n">cluster_centers</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span>

        <span class="n">dists</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">([</span><span class="n">A_q</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]],</span> <span class="p">[</span><span class="n">cluster_centers</span><span class="p">[</span><span class="n">c</span><span class="p">,</span> <span class="p">:]])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">dists</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="p">,</span> <span class="n">dist</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">indices_</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sorted</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">dists</span><span class="o">.</span><span class="n">values</span><span class="p">()]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">features_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">X</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_</span><span class="p">]</span>

<span class="c1"># Testing</span>
<span class="n">pfa</span> <span class="o">=</span> <span class="n">PFA</span><span class="p">(</span><span class="n">diff_n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">explained_var</span><span class="o">=</span> <span class="mf">0.95</span><span class="p">)</span>
<span class="n">pfa</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">regprops</span><span class="p">)</span>

<span class="n">featurekeys</span> <span class="o">=</span> <span class="p">[</span><span class="n">regprops</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pfa</span><span class="o">.</span><span class="n">indices_</span><span class="p">]</span>
<span class="n">featurekeys</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[&#39;area&#39;,
 &#39;centroid-0&#39;,
 &#39;centroid-1&#39;,
 &#39;solidity&#39;,
 &#39;mean_intensity&#39;,
 &#39;image_stdev&#39;,
 &#39;touching neighbor count&#39;]
</pre></div>
</div>
<p>This is a modification of the provided code in that it tries to force you to choose a subset. If you don’t manually say what the subset is, it will get a subset that explains 95% of the variance or the amount of variance that you determine. As described in the paper, the number of clusters (and with that the number of features) should be larger than the subset of eigenvectors chosen. This is implemented by not determining the number of features but only setting how many more features than the subset should be chosen. Apart from this a transform function was added as well as a fit_transform function, similarly to other scikit learn functions.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ryan_savill/principal_feature_analysis"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../../robert_haase/ms_build_tools/readme.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Installing Microsoft buildtools on Windows</p>
      </div>
    </a>
    <a class="right-next"
       href="../03_background_subtraction/readme.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Background Subtraction</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Marcelo Zoccoler, Johannes Müller, Till Korten, Ryan Savill, Mara Lampert, Robert Haase, DFG Cluster of Excellence "Physics of Life", TU Dresden
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>